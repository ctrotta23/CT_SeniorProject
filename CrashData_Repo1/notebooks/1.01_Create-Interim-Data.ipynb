{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to create the interim data source, and to better understand every  variable. \n",
    "\n",
    "Any additional variables that are to be created (or other changes to the data source) will be added to THIS notebook. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#df = pd.read_csv(\"C:/Users/chris/Documents/Senior_Project/CT_SeniorProject/CrashData_Repo1/data/raw/raw_data.csv\")\n",
    "df_interim = pd.read_csv(\"C:/Users/chris/Documents/Senior_Project/CT_SeniorProject/CrashData_Repo1/data/interim/filtered_dataset.csv\")\n",
    "\n",
    "#print(df.head())\n",
    "#print(df.describe())\n",
    "#print(df.columns)\n",
    "#print(df.shape)\n",
    "#print(df.isnull().sum())\n",
    "#print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "Create the new interim data source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Temp\\ipykernel_8868\\2698223047.py:9: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['CRASH_DATE'] = pd.to_datetime(df['CRASH_DATE'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data saved to C:/Users/chris/Documents/Senior_Project/CT_SeniorProject/CrashData_Repo1/data/interim/filtered_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Read the raw data\n",
    "raw_data_path = \"C:/Users/chris/Documents/Senior_Project/CT_SeniorProject/CrashData_Repo1/data/raw/raw_data.csv\"\n",
    "df = pd.read_csv(raw_data_path)\n",
    "\n",
    "# Convert CRASH_DATE to datetime and extract the year\n",
    "df['CRASH_DATE'] = pd.to_datetime(df['CRASH_DATE'])\n",
    "df['CRASH_YEAR'] = df['CRASH_DATE'].dt.year\n",
    "\n",
    "# Filter out the years 2013-2017 and 2025\n",
    "#df_interim = df[~df['CRASH_YEAR'].isin([2013, 2014, 2015, 2016, 2017, 2025])]\n",
    "df_interim = df[~df['CRASH_YEAR'].isin([2013, 2014, 2015, 2016, 2017, 2025])].copy()\n",
    "\n",
    "# Write the filtered dataset to a new interim CSV file\n",
    "output_path = \"C:/Users/chris/Documents/Senior_Project/CT_SeniorProject/CrashData_Repo1/data/interim/filtered_dataset.csv\"\n",
    "df_interim.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Filtered data saved to {output_path}\")\n",
    "\n",
    "#distinct_years = df_interim['CRASH_YEAR_X'].unique()\n",
    "#print(distinct_years)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023 2019 2022 2020 2021 2018 2024]\n"
     ]
    }
   ],
   "source": [
    "#Test to see if the data was filtered correctly\n",
    "print(df_interim['CRASH_YEAR'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following section is to view all unique values for the categorical data varaiables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u_posted_speed_limit:  [30 50 15 25 10 35 20 55 45  5 40  0 60  3 65 39 22 70 18 24  1 34 14 23\n",
      " 11 26  2  9 32 12 33  7 36  6 49  8 29 44 62  4 31 46 38 16]\n",
      "u_traffic_control_device:  ['TRAFFIC SIGNAL' 'NO CONTROLS' 'OTHER' 'UNKNOWN' 'OTHER WARNING SIGN'\n",
      " 'STOP SIGN/FLASHER' 'PEDESTRIAN CROSSING SIGN' 'OTHER REG. SIGN' 'YIELD'\n",
      " 'LANE USE MARKING' 'RAILROAD CROSSING GATE' 'FLASHING CONTROL SIGNAL'\n",
      " 'SCHOOL ZONE' 'POLICE/FLAGMAN' 'DELINEATORS' 'OTHER RAILROAD CROSSING'\n",
      " 'RR CROSSING SIGN' 'NO PASSING' 'BICYCLE CROSSING SIGN']\n",
      "u_device_condition:  ['FUNCTIONING PROPERLY' 'NO CONTROLS' 'FUNCTIONING IMPROPERLY' 'UNKNOWN'\n",
      " 'OTHER' 'NOT FUNCTIONING' 'WORN REFLECTIVE MATERIAL' 'MISSING']\n",
      "u_weather_condition:  ['CLEAR' 'SNOW' 'RAIN' 'UNKNOWN' 'CLOUDY/OVERCAST' 'BLOWING SNOW'\n",
      " 'FREEZING RAIN/DRIZZLE' 'OTHER' 'SEVERE CROSS WIND GATE' 'SLEET/HAIL'\n",
      " 'FOG/SMOKE/HAZE' 'BLOWING SAND, SOIL, DIRT']\n",
      "u_lighting_condition:  ['DUSK' 'DARKNESS, LIGHTED ROAD' 'DAYLIGHT' 'DARKNESS' 'UNKNOWN' 'DAWN']\n",
      "u_first_crash_type:  ['ANGLE' 'REAR END' 'PARKED MOTOR VEHICLE' 'SIDESWIPE SAME DIRECTION'\n",
      " 'PEDESTRIAN' 'FIXED OBJECT' 'TURNING' 'SIDESWIPE OPPOSITE DIRECTION'\n",
      " 'REAR TO FRONT' 'HEAD ON' 'REAR TO SIDE' 'PEDALCYCLIST' 'OTHER OBJECT'\n",
      " 'ANIMAL' 'REAR TO REAR' 'OTHER NONCOLLISION' 'OVERTURNED' 'TRAIN']\n",
      "u_trafficway_type:  ['FIVE POINT, OR MORE' 'DIVIDED - W/MEDIAN BARRIER'\n",
      " 'DIVIDED - W/MEDIAN (NOT RAISED)' 'NOT DIVIDED' 'OTHER' 'ONE-WAY'\n",
      " 'PARKING LOT' 'T-INTERSECTION' 'RAMP' 'FOUR WAY' 'UNKNOWN' 'ALLEY'\n",
      " 'UNKNOWN INTERSECTION TYPE' 'DRIVEWAY' 'TRAFFIC ROUTE' 'NOT REPORTED'\n",
      " 'CENTER TURN LANE' 'L-INTERSECTION' 'Y-INTERSECTION' 'ROUNDABOUT']\n",
      "u_lane_cnt:  [nan  2.  4.  0.  6.  1.  3. 11. 22. 16. 10.  5.  8. 13. 12. 15.  9.  7.\n",
      " 14. 41. 25. 99. 20. 30. 19. 21. 45. 60.]\n",
      "u_alignment:  ['STRAIGHT AND LEVEL' 'CURVE ON GRADE' 'CURVE, LEVEL' 'STRAIGHT ON GRADE'\n",
      " 'CURVE ON HILLCREST' 'STRAIGHT ON HILLCREST']\n",
      "u_roadway_surface_cond:  ['DRY' 'SNOW OR SLUSH' 'WET' 'UNKNOWN' 'OTHER' 'ICE' 'SAND, MUD, DIRT']\n",
      "u_road_defect:  ['NO DEFECTS' 'UNKNOWN' 'DEBRIS ON ROADWAY' 'OTHER' 'WORN SURFACE'\n",
      " 'SHOULDER DEFECT' 'RUT, HOLES']\n",
      "u_report_type:  ['ON SCENE' 'NOT ON SCENE (DESK REPORT)' nan]\n",
      "u_crash_type:  ['INJURY AND / OR TOW DUE TO CRASH' 'NO INJURY / DRIVE AWAY']\n",
      "u_intersection_rel:  ['Y' nan 'N']\n",
      "u_not_right_of_way:  [nan 'Y' 'N']\n",
      "u_hit_and_run:  [nan 'Y' 'N']\n",
      "u_damage:  ['OVER $1,500' '$501 - $1,500' '$500 OR LESS']\n",
      "u_prim_contributory_cause:  ['UNABLE TO DETERMINE' 'FOLLOWING TOO CLOSELY'\n",
      " 'FAILING TO REDUCE SPEED TO AVOID CRASH' 'FAILING TO YIELD RIGHT-OF-WAY'\n",
      " 'NOT APPLICABLE' 'WEATHER' 'IMPROPER BACKING'\n",
      " 'IMPROPER TURNING/NO SIGNAL' 'DRIVING SKILLS/KNOWLEDGE/EXPERIENCE'\n",
      " 'IMPROPER LANE USAGE'\n",
      " 'VISION OBSCURED (SIGNS, TREE LIMBS, BUILDINGS, ETC.)'\n",
      " 'ROAD ENGINEERING/SURFACE/MARKING DEFECTS'\n",
      " 'EQUIPMENT - VEHICLE CONDITION' 'IMPROPER OVERTAKING/PASSING'\n",
      " 'RELATED TO BUS STOP' 'DISREGARDING TRAFFIC SIGNALS'\n",
      " 'DRIVING ON WRONG SIDE/WRONG WAY' 'DISREGARDING ROAD MARKINGS'\n",
      " 'DISTRACTION - FROM INSIDE VEHICLE' 'ANIMAL'\n",
      " 'ROAD CONSTRUCTION/MAINTENANCE' 'TEXTING'\n",
      " 'CELL PHONE USE OTHER THAN TEXTING' 'DISREGARDING OTHER TRAFFIC SIGNS'\n",
      " 'DISREGARDING STOP SIGN'\n",
      " 'OPERATING VEHICLE IN ERRATIC, RECKLESS, CARELESS, NEGLIGENT OR AGGRESSIVE MANNER'\n",
      " 'DISTRACTION - FROM OUTSIDE VEHICLE' 'PHYSICAL CONDITION OF DRIVER'\n",
      " 'EXCEEDING SAFE SPEED FOR CONDITIONS' 'DISREGARDING YIELD SIGN'\n",
      " 'TURNING RIGHT ON RED' 'EXCEEDING AUTHORIZED SPEED LIMIT'\n",
      " 'EVASIVE ACTION DUE TO ANIMAL, OBJECT, NONMOTORIST'\n",
      " 'UNDER THE INFLUENCE OF ALCOHOL/DRUGS (USE WHEN ARREST IS EFFECTED)'\n",
      " 'HAD BEEN DRINKING (USE WHEN ARREST IS NOT MADE)'\n",
      " 'DISTRACTION - OTHER ELECTRONIC DEVICE (NAVIGATION DEVICE, DVD PLAYER, ETC.)'\n",
      " 'OBSTRUCTED CROSSWALKS' 'BICYCLE ADVANCING LEGALLY ON RED LIGHT'\n",
      " 'PASSING STOPPED SCHOOL BUS' 'MOTORCYCLE ADVANCING LEGALLY ON RED LIGHT']\n",
      "u_sec_contributory_cause:  ['NOT APPLICABLE' 'FOLLOWING TOO CLOSELY'\n",
      " 'OPERATING VEHICLE IN ERRATIC, RECKLESS, CARELESS, NEGLIGENT OR AGGRESSIVE MANNER'\n",
      " 'DISTRACTION - FROM INSIDE VEHICLE' 'UNABLE TO DETERMINE' 'WEATHER'\n",
      " 'FAILING TO YIELD RIGHT-OF-WAY' 'DRIVING SKILLS/KNOWLEDGE/EXPERIENCE'\n",
      " 'FAILING TO REDUCE SPEED TO AVOID CRASH' 'IMPROPER OVERTAKING/PASSING'\n",
      " 'IMPROPER TURNING/NO SIGNAL' 'DISREGARDING STOP SIGN'\n",
      " 'ROAD CONSTRUCTION/MAINTENANCE' 'IMPROPER LANE USAGE'\n",
      " 'DISTRACTION - FROM OUTSIDE VEHICLE' 'DISREGARDING TRAFFIC SIGNALS'\n",
      " 'DRIVING ON WRONG SIDE/WRONG WAY'\n",
      " 'VISION OBSCURED (SIGNS, TREE LIMBS, BUILDINGS, ETC.)'\n",
      " 'EQUIPMENT - VEHICLE CONDITION' 'IMPROPER BACKING'\n",
      " 'PHYSICAL CONDITION OF DRIVER' 'BICYCLE ADVANCING LEGALLY ON RED LIGHT'\n",
      " 'EXCEEDING AUTHORIZED SPEED LIMIT' 'EXCEEDING SAFE SPEED FOR CONDITIONS'\n",
      " 'DISTRACTION - OTHER ELECTRONIC DEVICE (NAVIGATION DEVICE, DVD PLAYER, ETC.)'\n",
      " 'UNDER THE INFLUENCE OF ALCOHOL/DRUGS (USE WHEN ARREST IS EFFECTED)'\n",
      " 'ROAD ENGINEERING/SURFACE/MARKING DEFECTS' 'DISREGARDING ROAD MARKINGS'\n",
      " 'DISREGARDING OTHER TRAFFIC SIGNS' 'ANIMAL'\n",
      " 'CELL PHONE USE OTHER THAN TEXTING'\n",
      " 'HAD BEEN DRINKING (USE WHEN ARREST IS NOT MADE)' 'TURNING RIGHT ON RED'\n",
      " 'PASSING STOPPED SCHOOL BUS' 'RELATED TO BUS STOP'\n",
      " 'EVASIVE ACTION DUE TO ANIMAL, OBJECT, NONMOTORIST'\n",
      " 'DISREGARDING YIELD SIGN' 'TEXTING'\n",
      " 'MOTORCYCLE ADVANCING LEGALLY ON RED LIGHT' 'OBSTRUCTED CROSSWALKS']\n",
      "u_beat_of_occurrence:  [ 225.  411. 1235. 1650. 1654. 1655. 1652. 1131.  424. 1814. 2221.  232.\n",
      " 1034. 1651.  222. 1211. 1653. 1712.  731. 1822. 2525.  423.  533.  331.\n",
      "  925.  824. 1811. 1121.  434.  412. 1023. 2511. 1433. 1214.  914. 2423.\n",
      " 1012. 2222. 1613. 1513.  431.  822.  825. 1031.  924.  833. 2521. 1913.\n",
      "  912.  811.  523. 1215.  323. 2512.  813.  723. 1522. 2032. 1011. 1622.\n",
      "  913. 1411. 2024. 2213. 2223. 1924. 1925.  834.  221. 2232. 1634. 1934.\n",
      "  334.  433.  711. 2522.  621. 1414.  622.  932.  122. 2413. 1424. 1022.\n",
      " 1621.  313.  132. 2523.  324.  111. 1033.  231. 1412. 1531. 1633. 1623.\n",
      " 1722. 2212.  421. 1112.  524. 1224.  414. 1013. 2234.  735. 1431.  921.\n",
      " 1834.  114. 1914. 2411. 1631.  224.  815. 1233. 2532. 1823. 2022.  835.\n",
      " 2513. 6100. 1912.  322. 1421. 1922. 2033. 1434. 1231. 1113.  614. 1021.\n",
      " 2023.  935.  321. 1135. 2432.  915. 1711.  623. 1733. 1123. 1221. 1122.\n",
      "  223. 1813. 2012. 1111.  312. 1533.  812. 1212.  311.  631. 2531. 1032.\n",
      " 1524.  432. 2533. 2524. 1724. 1222. 1911. 1432.  923. 1213. 1732.  821.\n",
      " 1611. 1832.  234. 2013.  611.  911. 2515. 1014.  934. 1821.  734. 2535.\n",
      "  511. 1225. 1413. 2514.  413. 1723. 2534.  131. 1134.  214.  722.  235.\n",
      " 1713.  712. 1632. 2412.  624.  332. 1133.  832. 1935.  422.  931. 1921.\n",
      " 1624. 1422.  634.  123. 1523.  112.  532.  124. 2233. 1234.  133. 2422.\n",
      " 1933.  726.  725.  233. 1731.  922.  933.  814. 1532. 1115. 1824.  733.\n",
      " 2211. 1232.  333.  714. 2011.  513. 1512.  512.  724. 1423.  632.  612.\n",
      "  613. 1132. 1932. 1614.  522.  215.  213. 1915. 1125.  212. 2431.  314.\n",
      " 2424. 1831. 1923. 1812. 1931. 1511. 1612.  211.  713.  823. 1833.  531.\n",
      "  633. 1024. 2031.  715. 2433. 1223. 1114.  121.  831. 1124.  732.  113.]\n",
      "u_photos_taken:  [nan 'Y' 'N']\n",
      "u_statements_taken:  [nan 'Y' 'N']\n",
      "u_dooring:  [nan 'Y' 'N']\n",
      "u_work_zone:  [nan 'Y' 'N']\n",
      "u_work_zone_type:  [nan 'CONSTRUCTION' 'UNKNOWN' 'MAINTENANCE' 'UTILITY']\n",
      "u_workers_present:  [nan 'Y' 'N']\n",
      "u_num_units:  [ 2  4  1  3  5  6  7  8 12  9 11 10 14 18 16 13 15]\n",
      "u_most_severe_injury:  ['INCAPACITATING INJURY' 'NO INDICATION OF INJURY'\n",
      " 'NONINCAPACITATING INJURY' 'FATAL' 'REPORTED, NOT EVIDENT' nan]\n",
      "u_injuries_total:  [ 3.  0.  1.  2.  5. nan  4.  6. 15.  7. 21.  8. 11. 17. 14.  9. 12. 10.\n",
      " 19. 13. 16.]\n",
      "u_injuries_fatal:  [ 0.  1. nan  2.  3.]\n",
      "u_injuries_incapacitating:  [ 1.  0.  5. nan  2.  3.  4.  6.  7. 10.  8.]\n",
      "u_injuries_non_incapacitating:  [ 2.  0.  1.  5. nan  3.  4.  6. 21. 10.  8.  7. 14. 12.  9. 19. 18. 11.\n",
      " 15. 16. 13.]\n",
      "u_injuries_reported_not_evident:  [ 0.  2. nan  1.  3.  5.  4. 10.  6.  7.  8. 11.  9. 15.]\n",
      "u_injuries_no_indication:  [ 2.  1.  5.  3.  0.  4.  8.  7. nan  6. 15. 11. 20. 18. 10.  9. 13. 12.\n",
      " 37. 16. 14. 22. 29. 30. 50. 21. 17. 42. 46. 24. 26. 61. 19. 48. 34. 31.\n",
      " 43. 45. 25. 40. 36. 23. 41. 28. 35. 32. 27. 33. 49.]\n",
      "u_injuries_unknown:  [ 0. nan]\n"
     ]
    }
   ],
   "source": [
    "#df_interim = pd.read_csv(\"C:/Users/chris/Documents/Senior_Project/CT_SeniorProject/CrashData_Repo1/data/interim/filtered_dataset.csv\")\n",
    "\n",
    "\n",
    "u_posted_speed_limit = df_interim['POSTED_SPEED_LIMIT'].unique()\n",
    "print(\"u_posted_speed_limit: \", u_posted_speed_limit)\n",
    "\n",
    "u_traffic_control_device = df_interim['TRAFFIC_CONTROL_DEVICE'].unique()\n",
    "print(\"u_traffic_control_device: \", u_traffic_control_device)\n",
    "\n",
    "u_device_condition = df_interim['DEVICE_CONDITION'].unique()\n",
    "print(\"u_device_condition: \", u_device_condition)\n",
    "\n",
    "u_weather_condition = df_interim['WEATHER_CONDITION'].unique()\n",
    "print(\"u_weather_condition: \", u_weather_condition)\n",
    "\n",
    "u_lighting_condition = df_interim['LIGHTING_CONDITION'].unique()\n",
    "print(\"u_lighting_condition: \", u_lighting_condition)\n",
    "\n",
    "u_first_crash_type = df_interim['FIRST_CRASH_TYPE'].unique()\n",
    "print(\"u_first_crash_type: \", u_first_crash_type)\n",
    "\n",
    "u_trafficway_type = df_interim['TRAFFICWAY_TYPE'].unique()\n",
    "print(\"u_trafficway_type: \", u_trafficway_type)\n",
    "\n",
    "u_lane_cnt = df_interim['LANE_CNT'].unique()\n",
    "print(\"u_lane_cnt: \", u_lane_cnt)\n",
    "\n",
    "u_alignment = df_interim['ALIGNMENT'].unique()\n",
    "print(\"u_alignment: \", u_alignment)\n",
    "\n",
    "u_roadway_surface_cond = df_interim['ROADWAY_SURFACE_COND'].unique()\n",
    "print(\"u_roadway_surface_cond: \", u_roadway_surface_cond)\n",
    "\n",
    "u_road_defect = df_interim['ROAD_DEFECT'].unique()\n",
    "print(\"u_road_defect: \", u_road_defect)\n",
    "\n",
    "u_report_type = df_interim['REPORT_TYPE'].unique()\n",
    "print(\"u_report_type: \", u_report_type)\n",
    "\n",
    "u_crash_type = df_interim['CRASH_TYPE'].unique()\n",
    "print(\"u_crash_type: \", u_crash_type)\n",
    "\n",
    "u_intersection_rel = df_interim['INTERSECTION_RELATED_I'].unique()\n",
    "print(\"u_intersection_rel: \", u_intersection_rel)\n",
    "\n",
    "u_not_right_of_way = df_interim['NOT_RIGHT_OF_WAY_I'].unique()\n",
    "print(\"u_not_right_of_way: \", u_not_right_of_way)\n",
    "\n",
    "u_hit_and_run = df_interim['HIT_AND_RUN_I'].unique()\n",
    "print(\"u_hit_and_run: \", u_hit_and_run)\n",
    "\n",
    "u_damage = df_interim['DAMAGE'].unique()\n",
    "print(\"u_damage: \", u_damage)\n",
    "\n",
    "u_prim_contributory_cause = df_interim['PRIM_CONTRIBUTORY_CAUSE'].unique()\n",
    "print(\"u_prim_contributory_cause: \", u_prim_contributory_cause)\n",
    "\n",
    "u_sec_contributory_cause = df_interim['SEC_CONTRIBUTORY_CAUSE'].unique()\n",
    "print(\"u_sec_contributory_cause: \", u_sec_contributory_cause)\n",
    "\n",
    "u_beat_of_occurrence = df_interim['BEAT_OF_OCCURRENCE'].unique()\n",
    "print(\"u_beat_of_occurrence: \", u_beat_of_occurrence)\n",
    "\n",
    "u_photos_taken = df_interim['PHOTOS_TAKEN_I'].unique()\n",
    "print(\"u_photos_taken: \", u_photos_taken)\n",
    "\n",
    "u_statements_taken = df_interim['STATEMENTS_TAKEN_I'].unique()\n",
    "print(\"u_statements_taken: \", u_statements_taken)\n",
    "\n",
    "u_dooring = df_interim['DOORING_I'].unique()\n",
    "print(\"u_dooring: \", u_dooring)\n",
    "\n",
    "u_work_zone = df_interim['WORK_ZONE_I'].unique()\n",
    "print(\"u_work_zone: \", u_work_zone)\n",
    "\n",
    "u_work_zone_type = df_interim['WORK_ZONE_TYPE'].unique()\n",
    "print(\"u_work_zone_type: \", u_work_zone_type)\n",
    "\n",
    "u_workers_present = df_interim['WORKERS_PRESENT_I'].unique()\n",
    "print(\"u_workers_present: \", u_workers_present)\n",
    "\n",
    "u_num_units = df_interim['NUM_UNITS'].unique()\n",
    "print(\"u_num_units: \", u_num_units)\n",
    "\n",
    "u_most_severe_injury = df_interim['MOST_SEVERE_INJURY'].unique()\n",
    "print(\"u_most_severe_injury: \", u_most_severe_injury)\n",
    "\n",
    "u_injuries_total = df_interim['INJURIES_TOTAL'].unique()\n",
    "print(\"u_injuries_total: \", u_injuries_total)\n",
    "\n",
    "u_injuries_fatal = df_interim['INJURIES_FATAL'].unique()\n",
    "print(\"u_injuries_fatal: \", u_injuries_fatal)\n",
    "\n",
    "u_injuries_incapacitating = df_interim['INJURIES_INCAPACITATING'].unique()\n",
    "print(\"u_injuries_incapacitating: \", u_injuries_incapacitating)\n",
    "\n",
    "u_injuries_non_incapacitating = df_interim['INJURIES_NON_INCAPACITATING'].unique()\n",
    "print(\"u_injuries_non_incapacitating: \", u_injuries_non_incapacitating)\n",
    "\n",
    "u_injuries_reported_not_evident = df_interim['INJURIES_REPORTED_NOT_EVIDENT'].unique()\n",
    "print(\"u_injuries_reported_not_evident: \", u_injuries_reported_not_evident)\n",
    "\n",
    "u_injuries_no_indication = df_interim['INJURIES_NO_INDICATION'].unique()\n",
    "print(\"u_injuries_no_indication: \", u_injuries_no_indication)\n",
    "\n",
    "u_injuries_unknown = df_interim['INJURIES_UNKNOWN'].unique()\n",
    "print(\"u_injuries_unknown: \", u_injuries_unknown)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformations:\n",
    "This notebook will be updated with other flags or variables that need to be added. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   CRASH_DAY_OF_WEEK CRASH_DAY_OF_WEEK_NAME\n",
      "0                  1                 Monday\n",
      "1                  1                 Monday\n",
      "2                  1                 Monday\n",
      "3                  1                 Monday\n",
      "4                  1                 Monday\n"
     ]
    }
   ],
   "source": [
    "# Create a new column for the day of week using CRASH_DAY_OF_WEEK\n",
    "#df_interim['CRASH_DAY_OF_WEEK'] = df_interim['CRASH_DATE'].dt.day_name()\n",
    "#print(df_interim['CRASH_DAY_OF_WEEK'].unique())\n",
    "\n",
    "# Reload the raw dataset\n",
    "# This step was because I wrote over the interim dataset previously with the day of week values \n",
    "# instead of creating a new column. So this step was to restore the interim dataset\n",
    "#raw_data_path = \"C:/Users/chris/Documents/Senior_Project/CT_SeniorProject/CrashData_Repo1/data/raw/raw_data.csv\"\n",
    "#df_raw = pd.read_csv(raw_data_path)\n",
    "\n",
    "# Check the CRASH_DAY_OF_WEEK column in the raw data\n",
    "#print(df_raw['CRASH_DAY_OF_WEEK'].unique())\n",
    "\n",
    "#print(df_raw[['CRASH_DATE', 'CRASH_DAY_OF_WEEK']].head())\n",
    "\n",
    "# Restore CRASH_DAY_OF_WEEK from the raw data\n",
    "#df_interim['CRASH_DAY_OF_WEEK'] = df_raw['CRASH_DAY_OF_WEEK']\n",
    "\n",
    "# Verify the restored column\n",
    "#print(df_interim[['CRASH_DATE', 'CRASH_DAY_OF_WEEK']].head())\n",
    "\n",
    "df_interim['CRASH_DATETIME'] = pd.to_datetime(df_interim['CRASH_DATETIME'], errors='coerce')\n",
    "\n",
    "# Extract correct day number (Monday = 1, Sunday = 7)\n",
    "df_interim['CRASH_DAY_OF_WEEK'] = df_interim['CRASH_DATETIME'].dt.dayofweek + 1  # Monday = 1, Sunday = 7\n",
    "\n",
    "# Extract correct day name\n",
    "df_interim['CRASH_DAY_OF_WEEK_NAME'] = df_interim['CRASH_DATETIME'].dt.day_name()\n",
    "\n",
    "print(df_interim[['CRASH_DAY_OF_WEEK', 'CRASH_DAY_OF_WEEK_NAME']].head())\n",
    "\n",
    "# Write the filtered dataset to a new interim CSV file\n",
    "output_path = \"C:/Users/chris/Documents/Senior_Project/CT_SeniorProject/CrashData_Repo1/data/interim/filtered_dataset.csv\"\n",
    "df_interim.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Filtered data saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRASH_DATETIME converted and split into CRASH_DATE and CRASH_TIME.\n"
     ]
    }
   ],
   "source": [
    "#df_interim = pd.read_csv(\"C:/Users/chris/Documents/Senior_Project/CT_SeniorProject/CrashData_Repo1/data/interim/filtered_dataset.csv\")\n",
    "\n",
    "# Rename CRASH_DATE to CRASH_DATETIME\n",
    "df_interim = df_interim.rename(columns={'CRASH_DATE': 'CRASH_DATETIME'})\n",
    "\n",
    "# Ensure CRASH_DATETIME is in datetime format\n",
    "df_interim['CRASH_DATETIME'] = pd.to_datetime(df_interim['CRASH_DATETIME'], errors='coerce')\n",
    "\n",
    "# Create separate CRASH_DATE and CRASH_TIME columns\n",
    "df_interim['CRASH_DATE'] = df_interim['CRASH_DATETIME'].dt.date  # Extract date\n",
    "df_interim['CRASH_TIME'] = df_interim['CRASH_DATETIME'].dt.time  # Extract time\n",
    "\n",
    "# Save updated data\n",
    "output_path = \"C:/Users/chris/Documents/Senior_Project/CT_SeniorProject/CrashData_Repo1/data/interim/filtered_dataset.csv\"\n",
    "df_interim.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"CRASH_DATETIME converted and split into CRASH_DATE and CRASH_TIME.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       CRASH_DATETIME  CRASH_DATE CRASH_TIME\n",
      "0 2023-09-05 19:05:00  2023-09-05   19:05:00\n",
      "1 2023-09-22 18:45:00  2023-09-22   18:45:00\n",
      "2 2023-07-29 14:45:00  2023-07-29   14:45:00\n",
      "3 2023-08-09 23:00:00  2023-08-09   23:00:00\n",
      "4 2023-08-18 12:50:00  2023-08-18   12:50:00\n"
     ]
    }
   ],
   "source": [
    "# Print head of data to ensure that CRASH_DATETIME was split correctly\n",
    "print(df_interim[['CRASH_DATETIME', 'CRASH_DATE', 'CRASH_TIME']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort all of the records in the dataset by CRASH_DATETIME in ascending order\n",
    "\n",
    "# Sort by CRASH_DATE\n",
    "df_interim = df_interim.sort_values(by='CRASH_DATETIME', ascending=True)\n",
    "\n",
    "# Save \n",
    "output_path = \"C:/Users/chris/Documents/Senior_Project/CT_SeniorProject/CrashData_Repo1/data/interim/filtered_dataset.csv\"\n",
    "df_interim.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        CRASH_DATETIME\n",
      "0  2018-01-01 00:00:00\n",
      "1  2018-01-01 00:00:00\n",
      "2  2018-01-01 00:00:00\n",
      "3  2018-01-01 00:05:00\n",
      "4  2018-01-01 00:07:00\n"
     ]
    }
   ],
   "source": [
    "# Print head of data to ensure that the data was sorted correctly\n",
    "print(df_interim[['CRASH_DATETIME']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       CRASH_DATETIME  Day_Of_Year\n",
      "0 2018-01-01 00:00:00            1\n",
      "1 2018-01-01 00:00:00            1\n",
      "2 2018-01-01 00:00:00            1\n",
      "3 2018-01-01 00:05:00            1\n",
      "4 2018-01-01 00:07:00            1\n"
     ]
    }
   ],
   "source": [
    "# Add Day_Of_Year column\n",
    "\n",
    "df_interim['CRASH_DATETIME'] = pd.to_datetime(df_interim['CRASH_DATETIME'], errors='coerce')\n",
    "\n",
    "# Add Day_Of_Year as a number (1 to 365/366)\n",
    "df_interim['Day_Of_Year'] = df_interim['CRASH_DATETIME'].dt.dayofyear\n",
    "\n",
    "# Save the updated dataset if needed\n",
    "output_path = \"C:/Users/chris/Documents/Senior_Project/CT_SeniorProject/CrashData_Repo1/data/interim/filtered_dataset.csv\"\n",
    "df_interim.to_csv(output_path, index=False)\n",
    "\n",
    "print(df_interim[['CRASH_DATETIME', 'Day_Of_Year']].head())  # Preview the result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n",
      "  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n",
      "  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n",
      "  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n",
      "  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n",
      "  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108\n",
      " 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126\n",
      " 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144\n",
      " 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162\n",
      " 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180\n",
      " 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198\n",
      " 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216\n",
      " 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234\n",
      " 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252\n",
      " 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270\n",
      " 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288\n",
      " 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306\n",
      " 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324\n",
      " 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342\n",
      " 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360\n",
      " 361 362 363 364 365 366]\n"
     ]
    }
   ],
   "source": [
    "# Check that the day_of_year values are correct\n",
    "print(df_interim['Day_Of_Year'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       CRASH_DATETIME  CRASH_DAY_OF_WEEK  Hour_Of_Day  Hour_Of_Week\n",
      "0 2018-01-01 00:00:00                  1            0             1\n",
      "1 2018-01-01 00:00:00                  1            0             1\n",
      "2 2018-01-01 00:00:00                  1            0             1\n",
      "3 2018-01-01 00:05:00                  1            0             1\n",
      "4 2018-01-01 00:07:00                  1            0             1\n"
     ]
    }
   ],
   "source": [
    "# Add Crash_Hour_Of_Week column 1-168\n",
    "\n",
    "# Ensure CRASH_DATETIME is in datetime format\n",
    "df_interim['CRASH_DATETIME'] = pd.to_datetime(df_interim['CRASH_DATETIME'], errors='coerce')\n",
    "\n",
    "# Extract hour of the day (0-23)\n",
    "df_interim['Hour_Of_Day'] = df_interim['CRASH_DATETIME'].dt.hour\n",
    "\n",
    "# Compute Hour of Week (Monday 12 AM = 1, Sunday 11 PM = 168)\n",
    "df_interim['Hour_Of_Week'] = ((df_interim['CRASH_DAY_OF_WEEK'] - 1) * 24) + df_interim['Hour_Of_Day'] + 1\n",
    "\n",
    "# Save the updated dataset\n",
    "output_path = \"C:/Users/chris/Documents/Senior_Project/CT_SeniorProject/CrashData_Repo1/data/interim/filtered_dataset.csv\"\n",
    "df_interim.to_csv(output_path, index=False)\n",
    "\n",
    "print(df_interim[['CRASH_DATETIME', 'CRASH_DAY_OF_WEEK', 'Hour_Of_Day', 'Hour_Of_Week']].head())  # Preview results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    CRASH_DAY_OF_WEEK CRASH_DAY_OF_WEEK_NAME\n",
      "0                   2                 Monday\n",
      "1                   2                 Monday\n",
      "2                   2                 Monday\n",
      "3                   2                 Monday\n",
      "4                   2                 Monday\n",
      "5                   2                 Monday\n",
      "6                   2                 Monday\n",
      "7                   2                 Monday\n",
      "8                   2                 Monday\n",
      "9                   2                 Monday\n",
      "10                  2                 Monday\n",
      "11                  2                 Monday\n",
      "12                  2                 Monday\n",
      "13                  2                 Monday\n",
      "14                  2                 Monday\n",
      "15                  2                 Monday\n",
      "16                  2                 Monday\n",
      "17                  2                 Monday\n",
      "18                  2                 Monday\n",
      "19                  2                 Monday\n",
      "20                  2                 Monday\n",
      "21                  2                 Monday\n",
      "22                  2                 Monday\n",
      "23                  2                 Monday\n",
      "24                  2                 Monday\n",
      "25                  2                 Monday\n",
      "26                  2                 Monday\n",
      "27                  2                 Monday\n",
      "28                  2                 Monday\n",
      "29                  2                 Monday\n",
      "30                  2                 Monday\n",
      "31                  2                 Monday\n",
      "32                  2                 Monday\n",
      "33                  2                 Monday\n",
      "34                  2                 Monday\n",
      "35                  2                 Monday\n",
      "36                  2                 Monday\n",
      "37                  2                 Monday\n",
      "38                  2                 Monday\n",
      "39                  2                 Monday\n",
      "40                  2                 Monday\n",
      "41                  2                 Monday\n",
      "42                  2                 Monday\n",
      "43                  2                 Monday\n",
      "44                  2                 Monday\n",
      "45                  2                 Monday\n",
      "46                  2                 Monday\n",
      "47                  2                 Monday\n",
      "48                  2                 Monday\n",
      "49                  2                 Monday\n"
     ]
    }
   ],
   "source": [
    "# Display the first 50 rows of the selected columns\n",
    "df_preview = df_interim[['CRASH_DAY_OF_WEEK', 'CRASH_DAY_OF_WEEK_NAME']].head(50)\n",
    "print(df_preview)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
